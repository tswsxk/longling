{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Example for using light module\n",
    "\n",
    "In this notebook, we will show how to train and evaluate a network in 5 minutes with friendly log information to both screen and local file.\n",
    "\n",
    "Before we use the light module, we need to\n",
    "1. prepare the training dataset\n",
    "2. define network\n",
    "3. define the parameters we are going to use in a configuration, such as the model_dir to store the training log and the hyper parameters of network.\n",
    "4. define how the model will be trained including:\n",
    "    * the loss function\n",
    "    * the fit procedure.\n",
    "    * the trainer (trainer in mxnet, optim in pytorch)\n",
    "5. we can additionally specify\n",
    "    * initialization method\n",
    "    * the evaluation including the data iterator and evaluation procedure\n",
    "    * hyper parameters searching with nni\n",
    "    * log to tensorboard\n",
    "    * command line interface\n",
    "\n",
    "For convenience, we call the five stages mentioned above as:\n",
    "1. data preparation\n",
    "2. network definition\n",
    "3. configuration\n",
    "4. training\n",
    "5. additional\n",
    "\n",
    "Furthermore, due to the implementation of these four parts are different in different frameworks,\n",
    "with loss of generality, we only show the examples of mxnet and pytorch. This notebook is a demo of mxnet,\n",
    "and the pytorch version can be found in [here](light_module_torch.ipynb)\n",
    "\n",
    "Ok, let us start."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Problem Statement and Data Preparation\n",
    "We start from a classic learning problem `recognizing hand-written digits` and use sklearn to get dataset,\n",
    "where the description of problem and dataset could be found in [here](https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html#sphx-glr-auto-examples-classification-plot-digits-classification-py)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 720x216 with 4 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))\n",
    "for ax, image, label in zip(axes, digits.images, digits.target):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    ax.set_title('Training: %i' % label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n        [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n        [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n        ...,\n        [ 0.,  4., 14., ..., 11.,  0.,  0.],\n        [ 0.,  0.,  0., ...,  5.,  0.,  0.],\n        [ 0.,  0., 12., ...,  8.,  0.,  0.]]),\n array([0, 1, 2, ..., 3, 4, 5]))"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# flatten the images\n",
    "n_samples = len(digits.images)\n",
    "data = digits.images.reshape((n_samples, -1))\n",
    "\n",
    "# split dataset into train, validation and test\n",
    "# we set (train, validation): test = 8 : 2 and train : validation = 9 : 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, digits.target, test_size=0.2, shuffle=False)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train, y_train, test_size=0.1, shuffle=False)\n",
    "\n",
    "X_train, y_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# transform data to mxnet compatible format\n",
    "from mxnet.gluon.data import ArrayDataset, DataLoader\n",
    "def transform(x, y, batch_size, **params):\n",
    "    dataset = ArrayDataset(x.astype(\"float32\"), y.astype(\"float32\"))\n",
    "    return DataLoader(dataset, batch_size=batch_size, **params)\n",
    "\n",
    "# because the batch_size and some other batch-relevant parameters are\n",
    "# supposed to be determined in configuration,\n",
    "# we will perform the real data transformation after we set the configuration."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Network Definition"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "\n",
    "class MLP(gluon.HybridBlock):\n",
    "    def __init__(self, hidden_layers=None, out=10, **kwargs):\n",
    "        super(MLP, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.nn = gluon.nn.HybridSequential()\n",
    "            if hidden_layers is not None:\n",
    "                for hidden_unit in hidden_layers:\n",
    "                    self.nn.add(\n",
    "                        gluon.nn.Dense(hidden_unit, activation=\"tanh\"),\n",
    "                        gluon.nn.Dropout(0.5)\n",
    "                    )\n",
    "            self.nn.add(\n",
    "                gluon.nn.Dense(out)\n",
    "            )\n",
    "    def hybrid_forward(self, F, x, *args, **kwargs):\n",
    "        return F.log_softmax(self.nn(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configuration"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "logger: <Logger mlp (INFO)>\nmodel_name: mlp\nmodel_dir: mlp\nbegin_epoch: 0\nend_epoch: 2\nbatch_size: 32\nsave_epoch: 1\noptimizer: Adam\noptimizer_params: {'learning_rate': 0.001, 'wd': 0.0001, 'clip_gradient': 1}\nlr_params: {}\ntrain_select: None\nsave_select: None\nctx: cpu(0)\ntoolbox_params: {}\nhyper_params: {'hidden_layers': [512, 128]}\ninit_params: {}\nloss_params: {}\ncaption: \nvalidation_result_file: mlp\\result.json\ncfg_path: mlp\\configuration.json"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from longling.ML.MxnetHelper.utils import Configuration\n",
    "\n",
    "configuration = Configuration(model_name=\"mlp\", model_dir=\"mlp\")\n",
    "configuration.end_epoch = 2\n",
    "configuration.batch_size = 32\n",
    "configuration.hyper_params = {\"hidden_layers\": [512, 128]}\n",
    "\n",
    "configuration"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import autograd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loss function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "SoftmaxCrossEntropyLoss(batch_axis=0, w=None)"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_f = gluon.loss.SoftmaxCELoss(from_logits=True)\n",
    "loss_f"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training Procedure"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def fit_f(_net, batch_size, batch_data, trainer, loss_function, ctx=mx.cpu(), *args, **kwargs):\n",
    "    with autograd.record():\n",
    "        x, y = batch_data\n",
    "        x = x.as_in_context(ctx)\n",
    "        y = y.as_in_context(ctx)\n",
    "        out = _net(x)\n",
    "        loss = loss_function(out, y)\n",
    "        loss.backward()\n",
    "    trainer.step(batch_size)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Light Module"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "<mxnet.gluon.data.dataloader.DataLoader at 0x164d7f790f0>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = transform(X_train, y_train, configuration.batch_size)\n",
    "train_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch| Total-E          Batch     Total-B       Loss-SoftmaxCrossEntropyLoss             Progress           \n",
      "    0|       1             41          41                           0.985927     [00:00<00:00, 69.30it/s]   \n",
      "Epoch| Total-E          Batch     Total-B       Loss-SoftmaxCrossEntropyLoss             Progress           \n",
      "    1|       1             41          41                           0.436568     [00:00<00:00, 80.12it/s]   \n"
     ]
    }
   ],
   "source": [
    "from longling.ML.MxnetHelper import light_module as lm\n",
    "\n",
    "net = MLP(**configuration.hyper_params)\n",
    "net.initialize(**configuration.init_params)\n",
    "\n",
    "trainer = gluon.Trainer(net.collect_params(), \"adam\", {\"learning_rate\": 0.001})\n",
    "\n",
    "lm.train(\n",
    "    net=net,\n",
    "    loss_function=loss_f,\n",
    "    cfg=configuration,\n",
    "    trainer=trainer,\n",
    "    train_data=train_data,\n",
    "    fit_f=fit_f,\n",
    "    initial_net=False\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation\n",
    "Then let us try to attach some evaluation procedure during training on the validation dataset:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "from longling.ML.metrics import classification_report\n",
    "def eval_f(_net, test_data, ctx=mx.cpu()):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for x, y in test_data:\n",
    "        x = x.as_in_context(ctx)\n",
    "        pred = _net(x).argmax(-1).asnumpy().tolist()\n",
    "        y_pred.extend(pred)\n",
    "        y_true.extend(y.asnumpy().tolist())\n",
    "\n",
    "    return classification_report(y_true, y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "<mxnet.gluon.data.dataloader.DataLoader at 0x164d7e91860>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data = transform(X_valid, y_valid, configuration.batch_size)\n",
    "valid_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch| Total-E          Batch     Total-B       Loss-SoftmaxCrossEntropyLoss             Progress           \n",
      "    0|       1             41          41                           1.013248     [00:00<00:00, 68.95it/s]   \n",
      "Epoch [0]\tLoss - SoftmaxCrossEntropyLoss: 1.013248\n",
      "           precision    recall        f1  support\n",
      "0.0         0.937500  1.000000  0.967742       15\n",
      "1.0         0.777778  0.933333  0.848485       15\n",
      "2.0         1.000000  0.714286  0.833333       14\n",
      "3.0         1.000000  0.785714  0.880000       14\n",
      "4.0         0.933333  1.000000  0.965517       14\n",
      "5.0         1.000000  0.857143  0.923077       14\n",
      "6.0         0.937500  1.000000  0.967742       15\n",
      "7.0         1.000000  1.000000  1.000000       15\n",
      "8.0         1.000000  0.714286  0.833333       14\n",
      "9.0         0.619048  0.928571  0.742857       14\n",
      "macro_avg   0.920516  0.893333  0.896209      144\n",
      "accuracy: 0.895833\n",
      "Epoch| Total-E          Batch     Total-B       Loss-SoftmaxCrossEntropyLoss             Progress           \n",
      "    1|       1             41          41                           0.443961     [00:00<00:00, 57.94it/s]   \n",
      "Epoch [1]\tLoss - SoftmaxCrossEntropyLoss: 0.443961\n",
      "           precision    recall        f1  support\n",
      "0.0         1.000000  1.000000  1.000000       15\n",
      "1.0         1.000000  0.866667  0.928571       15\n",
      "2.0         1.000000  0.857143  0.923077       14\n",
      "3.0         1.000000  1.000000  1.000000       14\n",
      "4.0         0.933333  1.000000  0.965517       14\n",
      "5.0         1.000000  0.857143  0.923077       14\n",
      "6.0         0.937500  1.000000  0.967742       15\n",
      "7.0         1.000000  1.000000  1.000000       15\n",
      "8.0         1.000000  1.000000  1.000000       14\n",
      "9.0         0.777778  1.000000  0.875000       14\n",
      "macro_avg   0.964861  0.958095  0.958298      144\n",
      "accuracy: 0.958333\n"
     ]
    }
   ],
   "source": [
    "net = MLP(**configuration.hyper_params)\n",
    "net.initialize(**configuration.init_params)\n",
    "\n",
    "trainer = gluon.Trainer(net.collect_params(), \"adam\", {\"learning_rate\": 0.001})\n",
    "\n",
    "lm.train(\n",
    "    net=net,\n",
    "    loss_function=loss_f,\n",
    "    cfg=configuration,\n",
    "    trainer=trainer,\n",
    "    train_data=train_data,\n",
    "    test_data=valid_data,\n",
    "    fit_f=fit_f,\n",
    "    eval_f=eval_f,\n",
    "    initial_net=False\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You may want to use tqdm to show the progress:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "lm.train(\n",
    "    net=net,\n",
    "    loss_function=loss_f,\n",
    "    cfg=configuration,\n",
    "    trainer=trainer,\n",
    "    train_data=train_data,\n",
    "    test_data=valid_data,\n",
    "    fit_f=fit_f,\n",
    "    eval_f=eval_f,\n",
    "    progress_monitor=\"tqdm\",\n",
    "    initial_net=False\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 29,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0: 100%|██████████| 41/41 [00:00<00:00, 131.91it/s]\n",
      "Epoch: 1: 100%|██████████| 41/41 [00:00<00:00, 134.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0]\tLoss - SoftmaxCrossEntropyLoss: 0.313531\n",
      "           precision    recall        f1  support\n",
      "0.0         1.000000  1.000000  1.000000       15\n",
      "1.0         0.882353  1.000000  0.937500       15\n",
      "2.0         1.000000  0.785714  0.880000       14\n",
      "3.0         0.933333  1.000000  0.965517       14\n",
      "4.0         1.000000  1.000000  1.000000       14\n",
      "5.0         1.000000  0.928571  0.962963       14\n",
      "6.0         0.937500  1.000000  0.967742       15\n",
      "7.0         1.000000  1.000000  1.000000       15\n",
      "8.0         1.000000  0.857143  0.923077       14\n",
      "9.0         0.875000  1.000000  0.933333       14\n",
      "macro_avg   0.962819  0.957143  0.957013      144\n",
      "accuracy: 0.958333\n",
      "Epoch [1]\tLoss - SoftmaxCrossEntropyLoss: 0.182917\n",
      "           precision    recall        f1  support\n",
      "0.0         1.000000  1.000000  1.000000       15\n",
      "1.0         1.000000  1.000000  1.000000       15\n",
      "2.0         1.000000  0.928571  0.962963       14\n",
      "3.0         0.933333  1.000000  0.965517       14\n",
      "4.0         0.933333  1.000000  0.965517       14\n",
      "5.0         1.000000  0.857143  0.923077       14\n",
      "6.0         0.937500  1.000000  0.967742       15\n",
      "7.0         1.000000  1.000000  1.000000       15\n",
      "8.0         1.000000  1.000000  1.000000       14\n",
      "9.0         1.000000  1.000000  1.000000       14\n",
      "macro_avg   0.980417  0.978571  0.978482      144\n",
      "accuracy: 0.979167\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simplest"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch| Total-E          Batch     Total-B       Loss-SoftmaxCrossEntropyLoss             Progress           \n",
      "    0|       1             41          41                           1.008341     [00:00<00:00, 60.06it/s]   \n",
      "Epoch [0]\tLoss - SoftmaxCrossEntropyLoss: 1.008341\n",
      "           precision    recall        f1  support\n",
      "0.0         0.750000  1.000000  0.857143       15\n",
      "1.0         0.812500  0.866667  0.838710       15\n",
      "2.0         1.000000  0.928571  0.962963       14\n",
      "3.0         1.000000  1.000000  1.000000       14\n",
      "4.0         0.916667  0.785714  0.846154       14\n",
      "5.0         1.000000  0.857143  0.923077       14\n",
      "6.0         0.937500  1.000000  0.967742       15\n",
      "7.0         1.000000  1.000000  1.000000       15\n",
      "8.0         1.000000  0.714286  0.833333       14\n",
      "9.0         0.812500  0.928571  0.866667       14\n",
      "macro_avg   0.922917  0.908095  0.909579      144\n",
      "accuracy: 0.909722\n",
      "Epoch| Total-E          Batch     Total-B       Loss-SoftmaxCrossEntropyLoss             Progress           \n",
      "    1|       1             41          41                            0.43618     [00:00<00:00, 52.19it/s]   \n",
      "Epoch [1]\tLoss - SoftmaxCrossEntropyLoss: 0.43618\n",
      "           precision    recall        f1  support\n",
      "0.0         0.937500  1.000000  0.967742       15\n",
      "1.0         0.937500  1.000000  0.967742       15\n",
      "2.0         1.000000  0.857143  0.923077       14\n",
      "3.0         1.000000  1.000000  1.000000       14\n",
      "4.0         0.933333  1.000000  0.965517       14\n",
      "5.0         1.000000  0.857143  0.923077       14\n",
      "6.0         0.937500  1.000000  0.967742       15\n",
      "7.0         1.000000  1.000000  1.000000       15\n",
      "8.0         0.933333  1.000000  0.965517       14\n",
      "9.0         1.000000  0.928571  0.962963       14\n",
      "macro_avg   0.967917  0.964286  0.964338      144\n",
      "accuracy: 0.965278\n"
     ]
    }
   ],
   "source": [
    "from longling.ML.MxnetHelper import fit_wrapper, loss_dict2tmt_mx_loss\n",
    "\n",
    "@fit_wrapper\n",
    "def fit_f(_net, batch_data, loss_function, *args, **kwargs):\n",
    "    x, y = batch_data\n",
    "    out = _net(x)\n",
    "    loss = loss_function(out, y)\n",
    "    return loss\n",
    "\n",
    "def get_net(*args, **kwargs):\n",
    "    return MLP(*args, **kwargs)\n",
    "\n",
    "def get_loss(*args, **kwargs):\n",
    "    return loss_dict2tmt_mx_loss(\n",
    "        {\"cross entropy\": gluon.loss.SoftmaxCELoss(*args, **kwargs)}\n",
    "    )\n",
    "\n",
    "lm.train(\n",
    "    net=None,\n",
    "    cfg=configuration,\n",
    "    loss_function=loss_f,\n",
    "    trainer=None,\n",
    "    train_data=train_data,\n",
    "    test_data=valid_data,\n",
    "    fit_f=fit_f,\n",
    "    eval_f=eval_f,\n",
    "    initial_net=True,\n",
    "    get_net=get_net,\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Discussion\n",
    "\n",
    "Even though light module provides a easy and fast way to xxx, which is quite similar to xxx\n",
    "(TBA, without the consideration of xxx, I will prefer to use for xxx).\n",
    "However, such approach still has its own shortcomings:\n",
    "* Inner Blackness: it is hard for users\n",
    "* Version Control: Even though, still unstable\n",
    "\n",
    "Therefore, we may need another one type of module to overcome the mentioned above.\n",
    "We hope the features this kind of module can provide:\n",
    "1. Details\n",
    "2. Stability\n",
    "\n",
    "For this purpose, I and propose a heavier module named template module.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}