{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction to MetaTools\n",
    "\n",
    "This tutorial will show how to quickly wrap a machine learning model with our GlueMetaTools,\n",
    "which can easily and fastly add a lot of assisting functions on the model such as loss monitoring, log system.\n",
    "\n",
    "Before we step into our development of a machine learning model,\n",
    "we will spend a little time in talking about the scenarios a model may be in\n",
    "and what will be done in different scenarios.\n",
    "After analyzing the needed procedure,\n",
    "we will summarize the needed components and propose our solution, i.e., `MetaTools`.\n",
    "\n",
    "With loss of generality, a machine learning model can be involved in these scenarios:\n",
    "1. **Training**: In this scenario, model will be trained based on the datasets with or without labels,\n",
    "and the datasets could be stored in local file system or database, even some interactive systems like `gym`.\n",
    "During training, losses and metrics on train dataset or valid dataset are\n",
    "expected to be printed to console and saved in a log file, which helps people\n",
    "to analyze the model effectiveness and find the best result.\n",
    "After training, the model parameters should be saved.\n",
    "In practice, people may save the model parameters every few epochs rather than only save in the end.\n",
    "In addition, auto machine learning now is widely used to search the best learning strategy\n",
    "(e.g., optimize strategy and learning rate), the best hyper-parameters and best network structure\n",
    "(if you are using deep learning technologies), which also is expected to be involved\n",
    "(in other words, auto machine learning is expected to be able to plugin into the model).\n",
    "Also, these things could be done manually.\n",
    "2. **Evaluation**: As known as \"validation\",\n",
    "people use the valid dataset to validate the effectiveness of the model based on several metrics.\n",
    "3. **Testing**: In fact, this scenario is quite like \"Evaluation\" in most research works :-),\n",
    "and to better distinguish \"Testing\" with \"Evaluation\", we prefer to call it \"Prediction\" or \"Submission\".\n",
    "In this scenario, people need to restore a model from existing parameters\n",
    "(including hyper-parameters and model-parameters).\n",
    "\n",
    "In summary, we will have the following steps when we are going to construct and utilize a machine learning model:\n",
    "1. Get data [Training, Evaluation, Testing]: Extract-Transform-Load (ETL) is a well-known data loading procedure, no matter train or test a model,\n",
    "this is needed. Usually, \"Training\" and \"Evaluation\" can share the same `etl`\n",
    "while \"Testing\" may need some modification.\n",
    "2. Persistent the hyper-parameters and model-parameters [Training].\n",
    "3. Log the losses and metrics during training into console and file system etc. [Training, Evaluation]\n",
    "4. Restore the stored hyper-parameters and model-parameters [Evaluation, Testing].\n",
    "5. Necessary hint info to let user know the progress (which step is the model on)[Training, Evaluation, Testing].\n",
    "6. Analyze the model effectiveness based on the logged losses and metrics,\n",
    "and find the best model based on this analysis.\n",
    "\n",
    "Therefore, we can know, we need to define these components in a `Configuration`\n",
    "and have several assisting tools to help monitor and analyze the model:\n",
    "1. The components in `Configuration`: (if you are not familiar with `Configuation`,\n",
    "you can refer to this tutorial[].)\n",
    "\n",
    "\n",
    "However, we notice that, there are still some variants brought by different machine learning framework,\n",
    "thus, for general frameworks,\n",
    "we propose `DLMeta` which can be used in most of machine learning frameworks like `sklearn` and `lightgbm`,\n",
    "with a little adaptive modification.\n",
    "and for specific frameworks, we propose `GLueMeta` for `mxnet.gluon` and `ToreMeta` for `pytorch`.\n",
    "\n",
    "We use the well-known problem \"Handwritten Digit Recognition\" as an example.\n",
    "\n",
    "First, we use `sklearn` to load the dataset and split it into `train` and `valid`:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "((1797, 64), (1797,))"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = load_digits()\n",
    "dataset.data.shape, dataset.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([ 0.,  0.,  0.,  9., 15.,  2.,  0.,  0.,  0.,  0.,  5., 16., 11.,\n         1.,  0.,  0.,  0.,  0., 13., 15.,  1.,  0.,  0.,  0.,  0.,  2.,\n        16., 11.,  0.,  0.,  0.,  0.,  0.,  2., 16., 11.,  4.,  4.,  0.,\n         0.,  0.,  2., 15., 16., 16., 14., 10.,  1.,  0.,  0.,  9., 16.,\n         7.,  3., 15.,  6.,  0.,  0.,  0.,  7., 15., 16., 16.,  6.]),\n (1437, 64),\n (1437,))"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.2, random_state=0)\n",
    "X_train[0], X_train.shape, y_train.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "((360, 64), (360,))"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, we batchify the dataset using `DataLoader` in `mxnet`, the tutorial of `DataLoader` can be\n",
    "found [here](https://mxnet.apache.org/versions/1.7/api/python/docs/tutorials/packages/gluon/data/datasets.html)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from mxnet.gluon.data import ArrayDataset\n",
    "\n",
    "train_dataset = ArrayDataset(X_train, y_train)\n",
    "valid_dataset = ArrayDataset(X_test, y_test)\n",
    "# train_data_loader = DataLoader(train_dataset, batch_size=16)\n",
    "# valid_data_loader = DataLoader(valid_dataset, batch_size=16)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, we found a configurable variable `batch_size`, so a `Configuration` is needed."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from mxnet import autograd\n",
    "from mxnet.gluon.loss import SoftmaxCELoss\n",
    "\n",
    "class FCNet(mx.gluon.HybridBlock):\n",
    "    def __init__(self, prefix=None, params=None):\n",
    "        super(FCNet, self).__init__(prefix=prefix, params=params)\n",
    "        with self.name_scope():\n",
    "            self.fc = mx.gluon.nn.HybridSequential()\n",
    "            self.fc.add(\n",
    "                mx.gluon.nn.Dense(256, \"relu\"),\n",
    "                mx.gluon.nn.Dropout(0.5),\n",
    "                mx.gluon.nn.Dense(8),\n",
    "            )\n",
    "    def hybrid_forward(self, F, x, *args, **kwargs):\n",
    "        return self.fc(x)\n",
    "\n",
    "Loss = SoftmaxCELoss\n",
    "\n",
    "def fit_f(net: FCNet, batch_data, bp_loss_f, trainer, *args, **kwargs):\n",
    "    feature, label = batch_data\n",
    "    with autograd.record():\n",
    "        pred = net(feature)\n",
    "        bp_loss = bp_loss_f(pred, label)\n",
    "        bp_loss.backward()\n",
    "        trainer.step(len(feature))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import functools\n",
    "from longling.ML.MxnetHelper.glue import MetaModule, MetaModel\n",
    "\n",
    "\n",
    "class Module(MetaModule):\n",
    "    @functools.wraps(FCNet.__init__)\n",
    "    def sym_gen(self, *args, **kwargs):\n",
    "        return FCNet(*args, **kwargs)\n",
    "\n",
    "    @functools.wraps(fit_f)\n",
    "    def fit_f(self, *args, **kwargs):\n",
    "        return fit_f(*args, **kwargs)\n",
    "\n",
    "\n",
    "class Model(MetaModel):\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for X_batch, y_batch in train_data_loader:\n",
    "#     print(\"X_batch has shape {}, and y_batch has shape {}\".format(X_batch.shape, y_batch.shape))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}